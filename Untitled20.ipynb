{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP19U7Emtii2vEB4PO+GDVW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/AdversarialModel/blob/master/Untitled20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##quantification\n",
        "L'objectif de ce notebook est d'apprendre un réseau qui pourra être utilisé en int8.\n",
        "\n",
        "### speed up\n",
        "Regardons d'abord quelle est l'accélération qu'on peut espérer."
      ],
      "metadata": {
        "id": "hfkvfsm8D6yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lPxYWtRTXYYC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    T = torch.zeros(100,5)\n",
        "    for i in range(100):\n",
        "        A = torch.randint(-64, 64, (2000,1, 128), dtype=torch.int8).cuda()\n",
        "        B = torch.randint(-64, 64, (1,1000, 128), dtype=torch.int8).cuda()\n",
        "\n",
        "        t0 = time.time()\n",
        "        C = torch.matmul(A[:,0,:].float(),B[0].float().t()).to(dtype=torch.int8)\n",
        "        tf = time.time()\n",
        "        T[i][0] = tf-t0\n",
        "\n",
        "        t0 = time.time()\n",
        "        C = torch._int_mm(A[:,0,:].to(dtype=torch.int8),B[0].t().to(dtype=torch.int8)).to(dtype=torch.int8)\n",
        "        tf = time.time()\n",
        "        T[i][1] = tf-t0\n",
        "\n",
        "        t0 = time.time()\n",
        "        C = (A.float()*B.float()).sum(-1).to(dtype=torch.int8)\n",
        "        tf = time.time()\n",
        "        T[i][2] = tf-t0\n",
        "\n",
        "        t0 = time.time()\n",
        "        C = (A.half()*B.half()).sum(-1,dtype=torch.half).to(dtype=torch.int8)\n",
        "        tf = time.time()\n",
        "        T[i][3] = tf-t0\n",
        "\n",
        "        t0 = time.time()\n",
        "        C = (A*B).sum(-1,dtype=torch.int8)\n",
        "        tf = time.time()\n",
        "        T[i][4] = tf-t0\n",
        "\n",
        "\n",
        "print(T.mean(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1phRiggfFsu1",
        "outputId": "7c39c722-67d4-4f89-ba0c-bd1eebc6ec4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0009, 0.0004, 0.0007, 0.0001, 0.0003])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "on observe que le gain à espérer n'est pas du tout impressionnant - pire, la variance est très forte (ça semble dépendre du contenu de la matrice).\n",
        "néanmoins, cela testons quand même.\n",
        "\n",
        "### architecture\n",
        "\n",
        "Le point central du réseau sera que\n",
        "- si x est de dimension 8 avec ||x||_oo <= 5\n",
        "- si ||w||_oo<=3\n",
        "- alors |wx| <= 3 x 5 x 8 = 120\n",
        "- donc activation(x)//16 est toujours un vecteur de dimension 8 avec ||x||_oo <= 5"
      ],
      "metadata": {
        "id": "TFrYkajBGYS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "on peut donc construire le module \"MyLinear\""
      ],
      "metadata": {
        "id": "V7ru9xxeJwTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(x):\n",
        "    if x.dtype==torch.int8:\n",
        "        return torch.clamp(x,-17,81)//16\n",
        "    else:\n",
        "        return torch.clamp(x,-17,81)/16\n",
        "\n",
        "class MyLinear(torch.nn.Module):\n",
        "    def __init__(self,size):\n",
        "        super(MyLinear, self).__init__()\n",
        "        assert size%8==0\n",
        "        self.size=size\n",
        "        self.data = torch.rand(1,size//8,8,8).cuda() * 6 - 3\n",
        "        self.bias = torch.rand(1,size).cuda() * 6 - 3\n",
        "        self.data = torch.nn.Parameter(self.data)\n",
        "        self.bias = torch.nn.Parameter(self.bias)\n",
        "        self.floatmode = True\n",
        "\n",
        "    def clamp(self):\n",
        "        with torch.no_grad():\n",
        "            torch.clamp(self.data,-3,3)\n",
        "            torch.clamp(self.bias,-3,3)\n",
        "\n",
        "    def goINT(self):\n",
        "        if self.floatmode:\n",
        "            self.clamp()\n",
        "            with torch.no_grad():\n",
        "                tmp = torch.round(self.data.clone()).to(dtype=torch.int8)\n",
        "                tmpbis = torch.round(self.bias.clone()).to(dtype=torch.int8)\n",
        "                del self.data, self.bias\n",
        "                self.data,self.bias = tmp,tmpbis\n",
        "                self.floatmode = False\n",
        "\n",
        "    def forward(self,x):\n",
        "        assert not (self.floatmode and x.dtype == torch.int8)\n",
        "        assert not (not self.floatmode and x.dtype != torch.int8)\n",
        "        assert x.shape[1]== self.size\n",
        "\n",
        "        x = x.view(x.shape[0],self.size//8,1,8)\n",
        "        x = (x*self.data).sum(-1,dtype=x.dtype)\n",
        "        x = activation(x.view(x.shape[0],self.size)+self.bias)\n",
        "        return x\n",
        "\n",
        "with torch.no_grad():\n",
        "    layer = MyLinear(32)\n",
        "    x = torch.rand(128,32).cuda()*10-5\n",
        "    t0=time.time()\n",
        "    y = layer(x)\n",
        "    t1=time.time()-t0\n",
        "    layer.goINT()\n",
        "    t0=time.time()\n",
        "    z = layer(x.to(dtype=torch.int8)).float()\n",
        "    t2 = time.time()-t0\n",
        "\n",
        "print((y-z).abs().sum(),t1,t2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb6r6JPoKLaf",
        "outputId": "06473713-6f47-418a-889e-6f538cc0a600"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2161.9700, device='cuda:0') 0.13850688934326172 0.044991254806518555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bon comme on a la flemme de faire une myconvolution, ben on va faire du transformer...\n",
        "(de toute façon, la différence de durée d'inférence ne motive pas à se casser les pieds !)\n",
        "\n",
        "### baseline\n",
        "construisons maintenant un petit réseau qui pourrait absorber des poids (en partie) int8"
      ],
      "metadata": {
        "id": "cNdXBNjo5UVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def channelPool(x):\n",
        "    return torch.max(x[:,::2],x[:,1::2])\n",
        "\n",
        "class SwinLike(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SwinLike, self).__init__()\n",
        "        self.proj = torch.nn.Conv2d(3,64,kernel_size=5,stride=3).cuda()\n",
        "\n",
        "        self.m1 = MyLinear(64*10*10)\n",
        "        self.m2 = MyLinear(32*10*10)\n",
        "        self.m3 = MyLinear(16*10*10)\n",
        "        self.m4 = MyLinear(8*10*10)\n",
        "        self.m5 = MyLinear(8*10*10)\n",
        "\n",
        "        self.final = torch.nn.Linear(800,10).cuda()\n",
        "        self.floatmode = True\n",
        "\n",
        "    def normalize(self):\n",
        "        self.m1.clamp()\n",
        "        self.m2.clamp()\n",
        "        self.m3.clamp()\n",
        "        self.m4.clamp()\n",
        "        self.m5.clamp()\n",
        "\n",
        "    def goINT(self):\n",
        "        if self.floatmode:\n",
        "            self.m1.goINT()\n",
        "            self.m2.goINT()\n",
        "            self.m3.goINT()\n",
        "            self.m4.goINT()\n",
        "            self.m5.goINT()\n",
        "            self.floatmode = False\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = activation(self.proj(x*5))\n",
        "        if not self.floatmode:\n",
        "            x = x.to(dtype=torch.int8)\n",
        "\n",
        "        x = channelPool(self.m1(x.flatten(1)))\n",
        "        x = channelPool(self.m2(x))\n",
        "        x = channelPool(self.m3(x))\n",
        "        x = self.m4(x)\n",
        "        x = self.m5(x)\n",
        "\n",
        "        return self.final(x.float())"
      ],
      "metadata": {
        "id": "udzmIEhtVmJE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"build\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"build\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "ogUioOi17-2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab5732b4-2ef1-431d-b5b2-f7959cc7d48b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net = SwinLike()\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "meanloss = []\n",
        "nb,nbOK = 0,0\n",
        "for i in range(60):\n",
        "    print(\"######\",i,\"######\")\n",
        "    for x,y in trainloader:\n",
        "        z = net(x.cuda())\n",
        "        loss = criterion(z,y.cuda().long())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        net.normalize()\n",
        "\n",
        "        floss = float(loss)\n",
        "        meanloss.append(floss)\n",
        "        _,z = z.max(1)\n",
        "        good = (y.cuda()==z).float()\n",
        "        nb+=good.shape[0]\n",
        "        nbOK+=good.sum().cpu().numpy()\n",
        "        if len(meanloss)==100:\n",
        "            print(sum(meanloss)/100, nbOK/nb)\n",
        "            meanloss=[]\n",
        "            # print([param for param in net.parameters()])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x,y in trainloader:\n",
        "            z = net(x.cuda())\n",
        "            _,z = z.max(1)\n",
        "            good = (y.cuda()==z).float()\n",
        "            nb+=good.shape[0]\n",
        "            nbOK+=good.sum().cpu().numpy()\n",
        "        print(\"eval :\",nbOK/nb)"
      ],
      "metadata": {
        "id": "uYSGXvGy9h_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3201dc72-9b13-4eff-da1b-a533cce02ba6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###### 0 ######\n",
            "2.3088514542579652 0.098671875\n",
            "2.3071426796913146 0.0991015625\n",
            "2.298610806465149 0.104140625\n",
            "eval : 0.13725\n",
            "###### 1 ######\n",
            "2.2576109623908995 0.13736752609933564\n",
            "2.1512867975234986 0.1445169896096602\n",
            "2.0662267327308657 0.15395417823781873\n",
            "2.010773991346359 0.16419685851868837\n",
            "eval : 0.19994\n",
            "###### 2 ######\n",
            "1.9862417376041412 0.20099948592217654\n",
            "1.9636357426643372 0.20614214519488247\n",
            "1.9359580147266389 0.21157153889356922\n",
            "1.8823267722129822 0.2175742821058229\n",
            "eval : 0.23976333333333333\n",
            "###### 3 ######\n",
            "1.8647804725170136 0.24071035010017927\n",
            "1.8288202226161956 0.24496610340989577\n",
            "1.7960936629772186 0.2496018914713605\n",
            "1.757036371231079 0.2544404661611907\n",
            "eval : 0.27396\n",
            "###### 4 ######\n",
            "1.718740541934967 0.2752985605820943\n",
            "1.6985238552093507 0.2788159306961055\n",
            "1.674166933298111 0.2826167807200238\n",
            "1.6479169404506684 0.2863108566888183\n",
            "eval : 0.301276\n",
            "###### 5 ######\n",
            "1.6136475265026093 0.3026633185700728\n",
            "1.619885141849518 0.30556348349274914\n",
            "1.599057264328003 0.3086080999698886\n",
            "1.586907238960266 0.3115719641281976\n",
            "eval : 0.323555\n",
            "###### 6 ######\n",
            "1.5720337414741516 0.32492025202994834\n",
            "1.5597806060314179 0.3273310828255706\n",
            "1.553568980693817 0.32972338864717193\n",
            "1.532821362018585 0.33211531786174747\n",
            "eval : 0.3418185714285714\n",
            "###### 7 ######\n",
            "1.5200767862796782 0.34318366701315134\n",
            "1.5123517966270448 0.34523987881209217\n",
            "1.5040196752548218 0.3472284315435949\n",
            "1.5026017069816588 0.34915012646289706\n",
            "eval : 0.3570675\n",
            "###### 8 ######\n",
            "1.4993139588832856 0.35830112108509965\n",
            "1.478337799310684 0.360132892790408\n",
            "1.4761069321632385 0.3618677648727384\n",
            "1.4729889678955077 0.36356439708547267\n",
            "eval : 0.3700822222222222\n",
            "###### 9 ######\n",
            "1.4646304440498352 0.37126854195226544\n",
            "1.4611755299568177 0.3727555547852612\n",
            "1.4552936244010926 0.3741858696023796\n",
            "1.439801540374756 0.37569142298222535\n",
            "eval : 0.381379\n",
            "###### 10 ######\n",
            "1.4338909351825715 0.3825520009490668\n",
            "1.4301111841201781 0.383855631052796\n",
            "1.4411904048919677 0.385130939524838\n",
            "1.4216470265388488 0.38636658031088084\n",
            "eval : 0.39114272727272725\n",
            "###### 11 ######\n",
            "1.421590131521225 0.39222160708636505\n",
            "1.4136701261997222 0.3934171618663103\n",
            "1.4157664370536804 0.3944716201400017\n",
            "eval : 0.3997983333333333\n",
            "###### 12 ######\n",
            "1.4048778569698335 0.39987960273899603\n",
            "1.40675656914711 0.40089831804281345\n",
            "1.4069318401813506 0.40187376082646353\n",
            "1.3905151677131653 0.40290328410616544\n",
            "eval : 0.40765\n",
            "###### 13 ######\n",
            "1.3951641845703124 0.40780739316344333\n",
            "1.386105546951294 0.4087557491543572\n",
            "1.392188640832901 0.4096346070421517\n",
            "1.3834186315536499 0.4105600876041343\n",
            "eval : 0.41468714285714287\n",
            "###### 14 ######\n",
            "1.3750066041946412 0.4149030020066585\n",
            "1.3768094086647034 0.4157639704885434\n",
            "1.3765037453174591 0.4165794217763246\n",
            "1.3590457618236542 0.417504550095441\n",
            "eval : 0.421258\n",
            "###### 15 ######\n",
            "1.3743712830543517 0.42152504519834094\n",
            "1.358960791826248 0.42230175050089636\n",
            "1.3479830992221833 0.4231268953257346\n",
            "1.3703905200958253 0.4238631649901483\n",
            "eval : 0.427311875\n",
            "###### 16 ######\n",
            "1.3590732824802398 0.42761043626434947\n",
            "1.3428624272346497 0.42830344432141726\n",
            "1.3485690343379975 0.42903891046767106\n",
            "1.3514873027801513 0.4297513673699782\n",
            "eval : 0.43283941176470586\n",
            "###### 17 ######\n",
            "1.3454496800899505 0.43317080544462566\n",
            "1.3407852780818938 0.43385376928373376\n",
            "1.3371658730506897 0.434515095960249\n",
            "1.3334554386138917 0.4351564075765077\n",
            "eval : 0.43796444444444443\n",
            "###### 18 ######\n",
            "1.3135279548168182 0.438357331232964\n",
            "1.3382952070236207 0.4389516107771802\n",
            "1.3139619958400726 0.4396128573423156\n",
            "1.3382455599308014 0.4401912761274221\n",
            "eval : 0.4427778947368421\n",
            "###### 19 ######\n",
            "1.3179543256759643 0.44319224676913793\n",
            "1.3046153128147124 0.44380109558933717\n",
            "1.3159529972076416 0.44440757372765016\n",
            "1.320257886648178 0.44495267750045187\n",
            "eval : 0.4472915\n",
            "###### 20 ######\n",
            "1.3201390290260315 0.44771320837312956\n",
            "1.300851514339447 0.44826350442897817\n",
            "1.3025005543231964 0.4488348789688777\n",
            "1.3025305819511415 0.44937617150890347\n",
            "eval : 0.4515838095238095\n",
            "###### 21 ######\n",
            "1.2851379275321961 0.4520695351692154\n",
            "1.3034170389175415 0.45255890239677016\n",
            "1.3086843073368073 0.4530714200146748\n",
            "1.2906089663505553 0.4535778345067802\n",
            "eval : 0.45560954545454546\n",
            "###### 22 ######\n",
            "1.285655015707016 0.4561129631772295\n",
            "1.2829823791980743 0.45661749374478733\n",
            "1.2844416558742524 0.45710195590632235\n",
            "eval : 0.45947652173913045\n",
            "###### 23 ######\n",
            "1.2873718678951263 0.45951403279418107\n",
            "1.2844370222091674 0.45994763356983803\n",
            "1.2663712513446808 0.46044996423806445\n",
            "1.2700007379055023 0.4608997749750352\n",
            "eval : 0.4631558333333333\n",
            "###### 24 ######\n",
            "1.2713753032684325 0.46322887802408613\n",
            "1.252639343738556 0.4637132440633945\n",
            "1.2815438520908355 0.46412453535273646\n",
            "1.2850121700763701 0.46452413655197733\n",
            "eval : 0.466648\n",
            "###### 25 ######\n",
            "1.248879632949829 0.46675974752317034\n",
            "1.2665664607286453 0.46715182829888713\n",
            "1.2529156613349914 0.46757711167352106\n",
            "1.262886346578598 0.4680181775259679\n",
            "eval : 0.4699676923076923\n",
            "###### 26 ######\n",
            "1.2458492922782898 0.4700985120290959\n",
            "1.251580795645714 0.47050992834959526\n",
            "1.2499946224689484 0.4709184806414718\n",
            "1.2401852333545684 0.47133631910977647\n",
            "eval : 0.4731711111111111\n",
            "###### 27 ######\n",
            "1.240698642730713 0.47334803422948185\n",
            "1.2390448582172393 0.4737505444571321\n",
            "1.2407895064353942 0.47414342331892156\n",
            "1.2500035190582275 0.47450785450219835\n",
            "eval : 0.47627107142857145\n",
            "###### 28 ######\n",
            "1.2178556656837463 0.476498723035527\n",
            "1.2442479157447814 0.47685652835156855\n",
            "1.2283501470088958 0.4772333433135988\n",
            "1.2315537023544312 0.4776296143204211\n",
            "eval : 0.47926551724137934\n",
            "###### 29 ######\n",
            "1.232094773054123 0.4794931439764936\n",
            "1.2114441978931427 0.47988809179458525\n",
            "1.235960268974304 0.4802512981487744\n",
            "1.222347890138626 0.48059098339289014\n",
            "eval : 0.4821993333333333\n",
            "###### 30 ######\n",
            "1.2130035966634751 0.4824683611613315\n",
            "1.2137308013439179 0.4828209387906386\n",
            "1.2195510947704316 0.48317120109669937\n",
            "1.2197138249874115 0.48352311508978263\n",
            "eval : 0.48500709677419357\n",
            "###### 31 ######\n",
            "1.2131692975759507 0.4852635532096593\n",
            "1.218773323893547 0.4856067029746595\n",
            "1.2088077998161315 0.4859610831606984\n",
            "1.1992003172636032 0.4863192517608318\n",
            "eval : 0.4877128125\n",
            "###### 32 ######\n",
            "1.1985382544994354 0.4880128198740434\n",
            "1.2063215529918672 0.48834080216769893\n",
            "1.2045843017101288 0.4886708246006011\n",
            "1.1986033284664155 0.4890099407200252\n",
            "eval : 0.49025454545454544\n",
            "###### 33 ######\n",
            "1.1865499609708785 0.4905981012046796\n",
            "1.2046678155660628 0.49091908615861346\n",
            "1.1912481373548507 0.4912229300278968\n",
            "eval : 0.4927873529411765\n",
            "###### 34 ######\n",
            "1.1963597691059114 0.492814270188381\n",
            "1.1902745598554612 0.493113949978439\n",
            "1.1788670986890792 0.4934499154790145\n",
            "1.187222289443016 0.4937551756703947\n",
            "eval : 0.4952425714285714\n",
            "###### 35 ######\n",
            "1.2007478189468384 0.4952877278749943\n",
            "1.190906475186348 0.4955908294259571\n",
            "1.1850410211086273 0.49589428266884383\n",
            "1.1788415038585662 0.49620655759931304\n",
            "eval : 0.49762805555555556\n",
            "###### 36 ######\n",
            "1.1786071544885635 0.4976975203382003\n",
            "1.181069700717926 0.4980145867995327\n",
            "1.162259999513626 0.4983409908638753\n",
            "1.173082773089409 0.4986101774227565\n",
            "eval : 0.49996351351351354\n",
            "###### 37 ######\n",
            "1.1847714537382126 0.5000674905189318\n",
            "1.1765793943405152 0.5003481279647374\n",
            "1.1602506893873215 0.5006501647262712\n",
            "1.169762219786644 0.5009351727557991\n",
            "eval : 0.5022321052631579\n",
            "###### 38 ######\n",
            "1.177307585477829 0.5023477312097412\n",
            "1.1650169372558594 0.5026363373506093\n",
            "1.159257241487503 0.5029261472794401\n",
            "1.162276102900505 0.503209604305766\n",
            "eval : 0.5044482051282051\n",
            "###### 39 ######\n",
            "1.1738975286483764 0.5045792581033594\n",
            "1.1596015733480454 0.5048620579854506\n",
            "1.1548675841093063 0.5051361501965348\n",
            "1.155774201154709 0.5054094776888197\n",
            "eval : 0.5065555\n",
            "###### 40 ######\n",
            "1.157675793170929 0.5067280820824018\n",
            "1.1477709114551544 0.507018564947469\n",
            "1.1541861414909362 0.507292575769597\n",
            "1.1577598923444747 0.5075482936570705\n",
            "eval : 0.5086568292682927\n",
            "###### 41 ######\n",
            "1.1371177732944489 0.5088689437776964\n",
            "1.16182841360569 0.5091196885117352\n",
            "1.1407643908262253 0.5093763786657998\n",
            "1.161105741262436 0.509625938457265\n",
            "eval : 0.5107192857142857\n",
            "###### 42 ######\n",
            "1.1569956332445144 0.5109104927714689\n",
            "1.1491529208421707 0.5111637251633045\n",
            "1.1317505997419357 0.51142180157447\n",
            "1.1381453198194504 0.5116797351651828\n",
            "eval : 0.5127174418604651\n",
            "###### 43 ######\n",
            "1.129996167421341 0.5129358016077433\n",
            "1.145993384718895 0.513165088474945\n",
            "1.143358690738678 0.5134075489031382\n",
            "1.1387459725141524 0.5136676647807951\n",
            "eval : 0.5146756818181818\n",
            "###### 44 ######\n",
            "1.1258416044712067 0.5149210568303791\n",
            "1.1284725505113602 0.5151825229238379\n",
            "1.1389681059122085 0.5154113848749675\n",
            "eval : 0.516548\n",
            "###### 45 ######\n",
            "1.1442099952697753 0.5165603114223755\n",
            "1.1122916024923324 0.5168066928994293\n",
            "1.1405324161052703 0.5170287037364346\n",
            "1.1323484528064727 0.517278323169657\n",
            "eval : 0.518390652173913\n",
            "###### 46 ######\n",
            "1.1339790523052216 0.5184273865485446\n",
            "1.1219881600141526 0.5186694728374687\n",
            "1.1220650017261504 0.5189065460630956\n",
            "1.1242685186862946 0.5191313204281203\n",
            "eval : 0.5202034042553192\n",
            "###### 47 ######\n",
            "1.1268435847759246 0.5202598627583063\n",
            "1.1207378196716309 0.5204839363629578\n",
            "1.1197325897216797 0.5207059509227364\n",
            "1.1202908486127854 0.5209373122895111\n",
            "eval : 0.5219339583333333\n",
            "###### 48 ######\n",
            "1.1196627670526504 0.5220020582436321\n",
            "1.1240650409460067 0.5222053787335247\n",
            "1.1192067497968674 0.5224252209662885\n",
            "1.1058826851844787 0.5226531937248889\n",
            "eval : 0.5236591836734694\n",
            "###### 49 ######\n",
            "1.1047954922914505 0.5237551699730574\n",
            "1.1157101160287857 0.5239651992009838\n",
            "1.1089371484518051 0.5241824529979427\n",
            "1.1028938508033752 0.5244014136928843\n",
            "eval : 0.5253674\n",
            "###### 50 ######\n",
            "1.1238277804851533 0.5254578139980824\n",
            "1.1055183112621307 0.525669429391138\n",
            "1.1028435617685317 0.5258885135135135\n",
            "1.0891123098134994 0.5261177846495401\n",
            "eval : 0.5270543137254902\n",
            "###### 51 ######\n",
            "1.1079967784881593 0.5271832768418216\n",
            "1.0962116062641143 0.5274045612489141\n",
            "1.1078195017576218 0.5276015594317098\n",
            "1.1003027957677842 0.5278018527961396\n",
            "eval : 0.5287176923076923\n",
            "###### 52 ######\n",
            "1.114075227379799 0.5288405714742094\n",
            "1.0861188977956773 0.5290513997499571\n",
            "1.09406922519207 0.5292596685251755\n",
            "1.0974169278144836 0.5294684458322153\n",
            "eval : 0.5302867924528302\n",
            "###### 53 ######\n",
            "1.100856090784073 0.5304334053503522\n",
            "1.0848803782463075 0.5306394025839731\n",
            "1.1076917004585267 0.530823982055142\n",
            "1.0807956123352052 0.5310362854732459\n",
            "eval : 0.5318698148148148\n",
            "###### 54 ######\n",
            "1.1080558490753174 0.532028782807196\n",
            "1.0790010952949525 0.5322343637532893\n",
            "1.0731251263618469 0.5324479896288273\n",
            "1.0963686114549638 0.532642811843048\n",
            "eval : 0.5334174545454545\n",
            "###### 55 ######\n",
            "1.0887253212928771 0.5336047937650577\n",
            "1.0960047888755797 0.5337756291448263\n",
            "1.078179685473442 0.533981248735951\n",
            "eval : 0.5349344642857143\n",
            "###### 56 ######\n",
            "1.0778414088487625 0.534942340985967\n",
            "1.0757073599100113 0.5351393259451818\n",
            "1.0728061878681183 0.5353199509714702\n",
            "1.088795669078827 0.5355018840513914\n",
            "eval : 0.5364264912280702\n",
            "###### 57 ######\n",
            "1.0845105135440827 0.5364507624440865\n",
            "1.0739077877998353 0.536641231793568\n",
            "1.0613178831338883 0.5368437704286025\n",
            "1.0880671626329421 0.5370307717823355\n",
            "eval : 0.5379284482758621\n",
            "###### 58 ######\n",
            "1.0813494849205016 0.5379717364810465\n",
            "1.068457362651825 0.5381622514278797\n",
            "1.0768431252241135 0.5383563904841384\n",
            "1.0715851587057115 0.5385349557352441\n",
            "eval : 0.5393910169491526\n",
            "###### 59 ######\n",
            "1.0732635480165482 0.5394514333411021\n",
            "1.073721416592598 0.5396307578732172\n",
            "1.0688399082422257 0.5398099827845806\n",
            "1.0679253202676773 0.5399951669098918\n",
            "eval : 0.540828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "with torch.no_grad():\n",
        "    for x,y in trainloader:\n",
        "        z = net(x.cuda())\n",
        "        _,z = z.max(1)\n",
        "        good = (y.cuda()==z).float()\n",
        "        nb+=good.shape[0]\n",
        "        nbOK+=good.sum().cpu().numpy()\n",
        "    print(\"eval :\",nbOK/nb,time.time()-t0)"
      ],
      "metadata": {
        "id": "KDSGq6aCxxQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70528a4a-cf2f-48e7-c738-2cae54cf2679"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval : 0.5415446280991736 7.491193771362305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time()\n",
        "net.goINT()\n",
        "with torch.no_grad():\n",
        "    for x,y in trainloader:\n",
        "        z = net(x.cuda())\n",
        "        _,z = z.max(1)\n",
        "        good = (y.cuda()==z).float()\n",
        "        nb+=good.shape[0]\n",
        "        nbOK+=good.sum().cpu().numpy()\n",
        "    print(\"eval :\",nbOK/nb,time.time()-t0)"
      ],
      "metadata": {
        "id": "gmWUYiCDyUGI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c113f07-cb18-4e02-ce22-0cdb3a356377"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval : 0.5381745901639344 7.794586658477783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ben c'est vraiment pas terrible :-( et c'est pas plus rapide ?????"
      ],
      "metadata": {
        "id": "QbDK6uagGyU_"
      }
    }
  ]
}