{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpLiqfEDMqapK7L2dsEbxG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/AdversarialModel/blob/master/Untitled21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### extraction de points d'intérêt\n",
        "\n",
        "Il existe déjà des modèles qui vont extraire de façon non supervisée des points d'intérêt dans des images comme DinoV2.\n",
        "\n",
        "On cherche ici à réaliser cette tache de façon explicite.\n",
        "\n",
        "-> on prend une image\n",
        "-> on crée une carte de features\n",
        "-> on voudrait que la classif se comporte \"aussi bien\" avec soit la carte de feature entière, soit k points seulement\n",
        "\n",
        "(ça peut être vit mais pas forcément)"
      ],
      "metadata": {
        "id": "qYmSevCp3VjW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Oos5ycG23Ux7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KeypointSelector(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(KeypointSelector, self).__init__()\n",
        "        self.encoder = torchvision.models.efficientnet_v2_s(weights=\"DEFAULT\").features\n",
        "        del self.encoder[3:]\n",
        "\n",
        "        self.rawclassif1 = torch.nn.Conv2d(48,4096,kernel_size=8)\n",
        "        self.rawclassif2 = torch.nn.Linear(4096,10)\n",
        "\n",
        "        self.projector = torch.nn.Conv2d(48,128,kernel_size=1)\n",
        "\n",
        "        self.selector1 = torch.nn.Conv2d(128,256,kernel_size=3,padding=1)\n",
        "        self.selector2 = torch.nn.Conv2d(128,1,kernel_size=1)\n",
        "\n",
        "        self.K = 4\n",
        "        self.fineclassif1 = torch.nn.Conv2d(512,4096,kernel_size=1)\n",
        "        self.fineclassif2 = torch.nn.Conv2d(4096,10,kernel_size=1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        with torch.no_grad():\n",
        "            f = self.encoder((x-0.5)*2)\n",
        "\n",
        "        praw = torch.nn.leaky_relu(self.rawclassif1(f))\n",
        "        praw = self.rawclassif2(praw)\n",
        "\n",
        "        f = torch.nn.leaky_relu(self.projector(f))\n",
        "        heatmap = torch.nn.leaky_relu(self.selector1(f))\n",
        "        heatmap = self.selector2(heatmap)\n",
        "        heatmap = heatmap.flatten(1)\n",
        "        _,I = torch.topk(heatmap, self.k, dim=1)\n",
        "\n",
        "        subf = torch.zeros(x.shape[0],128,self.k).cuda()\n",
        "        for i in range(self.k):\n",
        "            subf[:,:,i] = f[:,:,I[:,i]]\n",
        "\n",
        "        subf = subf.flatten(1)\n",
        "        pfine = torch.nn.leaky_relu(self.rawclassif1(subf))\n",
        "        pfine = self.rawclassif2(pfine)\n",
        "\n",
        "        maskedinput =\n",
        "        return praw, pfine\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "net = KeypointSelector()"
      ],
      "metadata": {
        "id": "nzBEteUL4o6M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"build\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"build\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor(),\n",
        ")\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "ogUioOi17-2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65f805e1-1954-4110-9175-b400eca4cb59"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in trainloader:\n",
        "    print(net(x).shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsSmPjPy6kDA",
        "outputId": "34e04729-6474-4343-e68e-1156f166f144"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 48, 8, 8])\n"
          ]
        }
      ]
    }
  ]
}