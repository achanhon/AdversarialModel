{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM35qC2u86Uw1pUDtkNtcgC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achanhon/AdversarialModel/blob/master/vitesse_tirage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIyIxhYKZOIj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = torch.rand(4)*4-2\n",
        "MAX,a = p.max(0)\n",
        "Mean = p.mean()\n",
        "target = 0.9*MAX+0.1*Mean\n",
        "print(p,MAX,Mean,target)"
      ],
      "metadata": {
        "id": "Hx2B-xj0ZRte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T0 = time.time()\n",
        "out = []\n",
        "for _ in range(100000):\n",
        "  if random.random()<0.9:\n",
        "    out.append(int(a))\n",
        "  else:\n",
        "    out.append(int(random.random()*4))\n",
        "T = time.time()\n",
        "print(T-T0)\n",
        "E = [p[i] for i in out]\n",
        "E = sum(E)/len(E)\n",
        "print(E)"
      ],
      "metadata": {
        "id": "OGYtSPFEZj_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft = torch.nn.functional.softmax(p,dim=0)\n",
        "softmean = (p*soft).sum()\n",
        "print(soft,softmean)\n",
        "assert softmean<target\n",
        "\n",
        "# seuil * MAX + (1-seuil)*softmean = target\n",
        "seuil = (target-softmean)/(MAX-softmean)\n",
        "\n",
        "T0 = time.time()\n",
        "out = []\n",
        "for _ in range(100000):\n",
        "  if random.random()<seuil:\n",
        "    out.append(int(a))\n",
        "  else:\n",
        "    out.append(int(torch.multinomial(soft, 1).item()))\n",
        "T = time.time()\n",
        "print(T-T0)\n",
        "E = [p[i] for i in out]\n",
        "E = sum(E)/len(E)\n",
        "print(E)"
      ],
      "metadata": {
        "id": "l16iaBARamYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net,self).__init__()\n",
        "\n",
        "    self.f1 = torch.nn.Linear(4,32)\n",
        "    self.f2 = torch.nn.Linear(32,1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.f2(torch.nn.functional.relu(self.f1(x)))\n",
        "\n",
        "R = torch.rand(10)*10-5\n",
        "X = torch.rand(10,4)\n",
        "\n",
        "print(\"option 1\")\n",
        "net = Net()\n",
        "optimizer = torch.optim.SGD(net.parameters(),lr=0.0005)\n",
        "log=[]\n",
        "for i in range(10000):\n",
        "  j = int(random.random()*10)\n",
        "  if j==9:\n",
        "    loss = (net(X[j])-R[j])**2\n",
        "  else:\n",
        "    loss = (net(X[j])-net(X[j+1])-R[j])**2\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i%100==99:\n",
        "    print(i,sum(log)/100)\n",
        "    log = []\n",
        "  else:\n",
        "    log.append(float(loss))\n",
        "\n",
        "print(\"option 2\")\n",
        "net = Net()\n",
        "optimizer = torch.optim.SGD(net.parameters(),lr=0.0005)\n",
        "log = []\n",
        "for i in range(10000):\n",
        "  j = int(random.random()*10)\n",
        "  if j==9:\n",
        "    loss = (net(X[j])-R[j])**2\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      NEXT = net(X[j+1])\n",
        "    loss = (net(X[j])-NEXT-R[j])**2\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i%100==99:\n",
        "    print(i,sum(log)/100)\n",
        "    log = []\n",
        "  else:\n",
        "    log.append(float(loss))"
      ],
      "metadata": {
        "id": "LCLtcVuXfFvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sans \"max\", il n'y a aucune raison de torch no_grad !!! mais avec, apparemment si..."
      ],
      "metadata": {
        "id": "LTTsSgD6iIHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R2QVJw5riHtF"
      }
    }
  ]
}